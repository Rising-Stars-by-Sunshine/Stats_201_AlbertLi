{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAWJU-PDjS6y"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment\n",
        "!pip install -q transformers\n",
        "!pip3 install emoji==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoConfig"
      ],
      "metadata": {
        "id": "Ea0D2mAXGbYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_file.csv' with the actual file name\n",
        "csv_file = '/content/reddit-labeled-output.csv'\n",
        "\n",
        "sentences = []\n",
        "\n",
        "with open(csv_file, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        # Assuming each row has at least 3 columns\n",
        "        sentences.append(row[1]+\". \"+row[2]) if len(row[2]) > 0 else sentences.append(row[1])  # Indexing starts from 0\n",
        "\n",
        "sentences = sentences[1:]"
      ],
      "metadata": {
        "id": "DpSHBvOsjgc9"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/cjhutto/vaderSentiment#python-demo-and-code-examples\n",
        "sentiments = []\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "for sentence in sentences:\n",
        "    vs = analyzer.polarity_scores(sentence)\n",
        "    sentiments.append(vs)"
      ],
      "metadata": {
        "id": "6v5zOvXEkQsM"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_sentiments = []\n",
        "for sentiment in sentiments:\n",
        "  val = sentiment['compound']\n",
        "  if val >= 0.05:\n",
        "    converted_sentiments.append(1)\n",
        "  elif val < 0.05 and val > -0.05:\n",
        "    converted_sentiments.append(0)\n",
        "  else:\n",
        "    converted_sentiments.append(-1)"
      ],
      "metadata": {
        "id": "rY-ibFw1mKbC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=f\"cardiffnlp/twitter-roberta-base-sentiment-latest\", tokenizer=AutoTokenizer.from_pretrained(f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"))\n",
        "\n",
        "sentiments_2 = []\n",
        "epoch = 1\n",
        "sequence_length = 128\n",
        "for sentence in sentences:\n",
        "  if epoch % 50 == 0:\n",
        "    print(f\"Epoch {epoch}\")\n",
        "  sentence_sentiment = []\n",
        "  iterations = int(len(sentence)/sequence_length+0.5)\n",
        "  for i in range(iterations-1):\n",
        "    sentence_sentiment.append(sentiment_task(sentence[sequence_length*i:sequence_length*(i+1)]))\n",
        "  sentence_sentiment.append(sentiment_task(sentence[sequence_length*(iterations-1):]))\n",
        "  sentiments_2.append(sentence_sentiment)\n",
        "  epoch += 1"
      ],
      "metadata": {
        "id": "XwmaDH7KpUmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateSentiment(sentiment):\n",
        "  sent_sum_pos = 0\n",
        "  sent_sum_neu = 0\n",
        "  sent_sum_neg = 0\n",
        "  for i in range(len(sentiment)):\n",
        "    if sentiment[i][0]['label'] == 'positive':\n",
        "      sent_sum_pos += sentiment[i][0]['score']\n",
        "    elif sentiment[i][0]['label'] == 'neutral':\n",
        "      sent_sum_neu += sentiment[i][0]['score']\n",
        "    else:\n",
        "      sent_sum_neg += sentiment[i][0]['score']\n",
        "\n",
        "  mag_score = max(sent_sum_pos,sent_sum_neu,sent_sum_neg)\n",
        "  sent = 0\n",
        "\n",
        "  if mag_score == sent_sum_pos:\n",
        "    sent = 1\n",
        "  elif mag_score == sent_sum_neu:\n",
        "    sent = 0\n",
        "  else:\n",
        "    sent = -1\n",
        "\n",
        "  return {'sent': sent,\n",
        "          'score': mag_score/len(sentiment)\n",
        "          }\n",
        "\n",
        "sentiment_results_2 = []\n",
        "for sentiment in sentiments_2:\n",
        "  sentiment_results_2.append(calculateSentiment(sentiment))\n",
        "\n",
        "converted_sentiments_2 = []\n",
        "for result in sentiment_results_2:\n",
        "  converted_sentiments_2.append(result['sent'])"
      ],
      "metadata": {
        "id": "VYYaFwB1MTGp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\n",
        "pipe = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
        "\n",
        "sentiments_3 = []\n",
        "epoch = 1\n",
        "sequence_length = 128\n",
        "for sentence in sentences:\n",
        "  if epoch % 50 == 0:\n",
        "    print(f\"Epoch {epoch}\")\n",
        "  sentence_sentiment = []\n",
        "  iterations = int(len(sentence)/sequence_length+0.5)\n",
        "  for i in range(iterations-1):\n",
        "    sentence_sentiment.append(pipe(sentence[sequence_length*i:sequence_length*(i+1)]))\n",
        "  sentence_sentiment.append(pipe(sentence[sequence_length*(iterations-1):]))\n",
        "  sentiments_3.append(sentence_sentiment)\n",
        "  epoch += 1"
      ],
      "metadata": {
        "id": "CkrkqkQSF2Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_results_3 = []\n",
        "for sentiment in sentiments_3:\n",
        "  sentiment_results_3.append(calculateSentiment(sentiment))\n",
        "\n",
        "converted_sentiments_3 = []\n",
        "for result in sentiment_results_3:\n",
        "  converted_sentiments_3.append(result['sent'])"
      ],
      "metadata": {
        "id": "oSQpb8QwKKBl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_file.csv' with the actual file name\n",
        "output_csv_file = '/content/reddit-processed-output.csv'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "df['s1'] = converted_sentiments\n",
        "df['s2'] = converted_sentiments_2\n",
        "df['s3'] = converted_sentiments_3\n",
        "\n",
        "# Write the DataFrame with the new column back to a CSV file\n",
        "df.to_csv(output_csv_file, index=False)"
      ],
      "metadata": {
        "id": "HqyIRLHgNS8Q"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}